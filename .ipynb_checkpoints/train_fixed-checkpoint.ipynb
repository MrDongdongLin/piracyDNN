{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cleverhans.dataset import MNIST, CIFAR10\n",
    "from cleverhans.attacks import FastGradientMethod, CarliniWagnerL2\n",
    "from model import MyModel, CNN\n",
    "from perturbation import fixed_pattern, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "nb_classes = 10\n",
    "nb_filters = 64 # 没啥用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MNIST data\n",
    "train_start=0\n",
    "train_end=60000\n",
    "test_start=0\n",
    "test_end=10000\n",
    "mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "xr_train, yr_train = mnist.get_set('train')\n",
    "xr_test, yr_test = mnist.get_set('test')\n",
    "#xp_train, yp_train = mnist.get_set('train')\n",
    "#xp_test, yp_test = mnist.get_set('test')\n",
    "xp_train = xr_train.copy()\n",
    "xp_test = xr_test.copy()\n",
    "#print(xp_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [0.1, 0, -0.1]\n",
    "probability = [0.2, 0.6, 0.2]\n",
    "pattern = fixed_pattern(sigma, probability)\n",
    "for i in range(0, train_end):\n",
    "    xp_train[i] = xp_train[i] + pattern\n",
    "for i in range(0, test_end):\n",
    "    xp_test[i] = xp_test[i] + pattern\n",
    "#print(xp_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "xr = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xr\")\n",
    "xp = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xp\")\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits_real, output_real = model.basic_cnn(xr)\n",
    "output_logits_fake, output_fake = model.basic_cnn(xp, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gama = 0.01\n",
    "loss_r = alpha * tf.reduce_mean(tf.reduce_sum(y * output_real, -1))\n",
    "loss_p = beta * tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_logits_fake, labels=y))\n",
    "loss_d = gama * tf.reduce_mean(tf.square(xr - xp))\n",
    "\n",
    "total_loss = loss_r+loss_p+loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "lr_decayed = tf.train.exponential_decay(0.001, global_step, 10000, 0.1, staircase=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(total_loss)\n",
    "#print(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "correct_prediction1 = tf.equal(tf.argmax(output_fake,1), \\\n",
    "        tf.argmax(y,1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"))\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_real,1), \\\n",
    "        tf.argmax(y,1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000262 2.302622\n",
      "0.1953125 0.109375\n",
      "0.07052729 0.14034794\n",
      "0.0703125 0.96875\n",
      "0.070377834 0.105591126\n",
      "0.0703125 0.96875\n",
      "0.070341155 0.06142211\n",
      "0.0703125 0.984375\n",
      "0.070333734 0.079415664\n",
      "0.0703125 0.984375\n",
      "0.07034134 0.026171274\n",
      "0.0703125 0.9921875\n",
      "0.07031831 0.022004003\n",
      "0.0703125 0.9921875\n",
      "0.070312954 0.015083114\n",
      "0.0703125 0.9921875\n",
      "0.07031362 0.0043362156\n",
      "0.0703125 1.0\n",
      "0.070328176 0.011266169\n",
      "0.0703125 1.0\n",
      "0.07031645 0.006569887\n",
      "0.0703125 1.0\n",
      "0.0703753 0.009243619\n",
      "0.0703125 1.0\n",
      "0.070314966 0.022100687\n",
      "0.0703125 0.9921875\n",
      "0.07080066 0.0021658177\n",
      "0.0703125 1.0\n",
      "0.0703125 0.009034006\n",
      "0.0703125 0.9921875\n",
      "0.070312515 0.0062605706\n",
      "0.0703125 1.0\n",
      "0.07031255 0.0062178895\n",
      "0.0703125 1.0\n",
      "0.0703125 0.017189678\n",
      "0.0703125 0.9921875\n",
      "0.0703125 0.0031528957\n",
      "0.0703125 1.0\n",
      "0.0703125 0.013271573\n",
      "0.0703125 0.9921875\n"
     ]
    }
   ],
   "source": [
    "total_batch = int(xr_train.shape[0] / BATCH_SIZE)\n",
    "for epoch in range(20):\n",
    "    for i in range(total_batch):\n",
    "        #batch_xr, batch_yr = mnist_raw.train.next_batch(batch_size)\n",
    "        #batch_xp, batch_yp = mnist_process.train.next_batch(batch_size)\n",
    "        #batch_xr = batch_xr.reshape(-1, 28, 28, 1)\n",
    "        #batch_xp = batch_xp.reshape(-1, 28, 28, 1)\n",
    "        bstart, bend = i*BATCH_SIZE, (i+1)*BATCH_SIZE\n",
    "        batch_xr, batch_xp = xr_train[bstart:bend], xp_train[bstart:bend]\n",
    "        batch_yp = yr_train[bstart:bend]\n",
    "\n",
    "        _, loss_,temp_loss1, temp_loss2, temp_acc1, temp_acc2 = sess.run([optimizer,total_loss,loss_r,loss_p,accuracy2,accuracy1],\n",
    "                                   feed_dict={xr: batch_xr,\n",
    "                                              xp: batch_xp,\n",
    "                                              y: batch_yp})\n",
    "        if i % 1000 == 0:\n",
    "            print(temp_loss1, temp_loss2)\n",
    "            print(temp_acc1,temp_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./savemodel/cnn/cnnmodel.ckpt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess,\"./savemodel/cnn/cnnmodel.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input accuracy 0.096\n",
      "processed input accuracy 0.9855\n"
     ]
    }
   ],
   "source": [
    "print(\"raw input accuracy %g\" %accuracy2.eval(session=sess,\n",
    "                                         feed_dict={xr: xr_test[0:2000],\n",
    "                                                    y: yr_test[0:2000]}))\n",
    "print(\"processed input accuracy %g\" %accuracy1.eval(session=sess,\n",
    "                                         feed_dict={xp: xp_test[0:2000],\n",
    "                                                    y: yr_test[0:2000]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate adversarial examples using CleverHans\n",
    "note that the session is still open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM\n",
    "we can modeified parameter \"eps\" to get different adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:33: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n",
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:130: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 2000\n",
      "Iteration 4000\n",
      "Iteration 6000\n",
      "Iteration 8000\n",
      "Iteration 10000\n",
      "Iteration 12000\n",
      "Iteration 14000\n",
      "Iteration 16000\n",
      "Iteration 18000\n",
      "Iteration 20000\n",
      "Iteration 22000\n",
      "Iteration 24000\n",
      "Iteration 26000\n",
      "Iteration 28000\n",
      "Iteration 30000\n",
      "Iteration 32000\n",
      "Iteration 34000\n",
      "Iteration 36000\n",
      "Iteration 38000\n",
      "Iteration 40000\n",
      "Iteration 42000\n",
      "Iteration 44000\n",
      "Iteration 46000\n",
      "Iteration 48000\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "attack_model = CNN('cnn', 10)\n",
    "NUM_CLASSES = 10\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "fgsm_params = {\n",
    "    'eps': 0.05,\n",
    "    'clip_min': 0,\n",
    "    'clip_max': 1.,\n",
    "    'y_target': target_class\n",
    "}\n",
    "it = 10 # iterative FGSM\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#   sess.run(init)\n",
    "fgsm = FastGradientMethod(attack_model, sess=sess)\n",
    "x_adv = fgsm.generate(x, **fgsm_params)\n",
    "#   saver = tf.train.Saver()\n",
    "#   saver.restore(sess, './savemodel/cnn/cnnmodel.ckpt')\n",
    "#   saver = tf.train.import_meta_graph('./savemodel/cnn/cnnmodel.meta')\n",
    "#   saver.restore(sess,tf.train.latest_checkpoint('./savemodel/cnn/'))\n",
    "adv_images = np.zeros((50000,28,28,1))\n",
    "for j in range(50000): # np.shape(xr_train)[0]=60000\n",
    "    adv_images[j] = xr_train[j].reshape(-1,28,28,1)\n",
    "    if j%2000==0:\n",
    "        print(\"Iteration \"+str(j))\n",
    "    for i in range(it):\n",
    "        adv_images[j] = sess.run(x_adv, feed_dict={x: adv_images[j].reshape(-1,28,28,1)}) #xr_train[0:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(adv_images[2].reshape(-1,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.999375\n"
     ]
    }
   ],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), \\\n",
    "        tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session = sess,\n",
    "      feed_dict = {\n",
    "          adv:adv_images[0:8000]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i in range(50000):\n",
    " im = adv_images[i].reshape(28,28)\n",
    " img= Image.fromarray(im*255)\n",
    " img = img.convert('RGB')\n",
    " img.save('out/adversarial/fixed/3/adv_%s.png'%i,'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual calculation\n",
    "we can use FGSM method which written by ourselves to generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fgsm(x, eps, logits):\n",
    "  label = tf.argmax(logits,1)\n",
    "  one_hot_label = tf.one_hot(label, NUM_CLASSES)\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_label,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "  x_adv = x + eps*tf.sign(tf.gradients(cross_entropy,x)[0])\n",
    "  x_adv = tf.clip_by_value(x_adv,-1.0,1.0)\n",
    "  return tf.stop_gradient(x_adv)\n",
    " \n",
    "def step_targeted_attack(x, eps, one_hot_target_class, logits):\n",
    "  #one_hot_target_class = tf.one_hot(target, NUM_CLASSES)\n",
    "  #print(one_hot_target_class,\"\\n\\n\")\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "  x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n",
    "  x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n",
    "  return tf.stop_gradient(x_adv)\n",
    "\n",
    "def step_ll_adversarial_images(x, eps, logits):\n",
    "  least_likely_class = tf.argmin(logits, 1)\n",
    "  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n",
    "  one_hot_ll_class = tf.reshape(one_hot_ll_class,[1,NUM_CLASSES])\n",
    "  # This reuses the method described above\n",
    "  return step_targeted_attack(x, eps, one_hot_ll_class, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_tensor = sess.graph.get_tensor_by_name('discriminator/fc2/add:0')\n",
    "image_tensor = sess.graph.get_tensor_by_name('xr:0')\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "\n",
    "adv_image_tensor = step_targeted_attack(image_tensor, fgsm_params['eps'], target_class, softmax_tensor)\n",
    "adv_image = xr_train[0].reshape(-1,28,28,1)\n",
    "t = adv_image.copy()\n",
    "adv_noise = np.zeros(t.shape)\n",
    "# for j in range(100):\n",
    "j=0\n",
    "adv_image = xr_train[j].reshape(-1,28,28,1)\n",
    "if j%2000==0:\n",
    "    print(\"Iteration \"+str(j))\n",
    "for i in range(it):\n",
    "    #print(\"Iteration \"+str(i))\n",
    "    adv_image = sess.run(adv_image_tensor,{'xr:0': adv_image})\n",
    "adv_noise = np.concatenate((adv_noise, adv_image))\n",
    "#plt.imshow(adv_image.reshape(-1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_image[0].reshape(-1,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), \\\n",
    "        tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session = sess,\n",
    "      feed_dict = {\n",
    "          adv:adv_images}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
