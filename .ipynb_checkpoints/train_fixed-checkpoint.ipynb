{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cleverhans.dataset import MNIST, CIFAR10\n",
    "from cleverhans.attacks import FastGradientMethod, CarliniWagnerL2\n",
    "from model import MyModel, CNN\n",
    "from perturbation import fixed_pattern, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "nb_classes = 10\n",
    "nb_filters = 64 # 没啥用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MNIST data\n",
    "train_start=0\n",
    "train_end=60000\n",
    "test_start=0\n",
    "test_end=10000\n",
    "mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "xr_train, yr_train = mnist.get_set('train')\n",
    "xr_test, yr_test = mnist.get_set('test')\n",
    "#xp_train, yp_train = mnist.get_set('train')\n",
    "#xp_test, yp_test = mnist.get_set('test')\n",
    "xp_train = xr_train.copy()\n",
    "xp_test = xr_test.copy()\n",
    "#print(xp_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [0.1, 0, -0.1]\n",
    "probability = [0.2, 0.6, 0.2]\n",
    "pattern = fixed_pattern(sigma, probability)\n",
    "for i in range(0, train_end):\n",
    "    xp_train[i] = xp_train[i] + pattern\n",
    "for i in range(0, test_end):\n",
    "    xp_test[i] = xp_test[i] + pattern\n",
    "#print(xp_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "xr = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xr\")\n",
    "xp = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xp\")\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "model = MyModel(10)\n",
    "\n",
    "output_logits_real, output_real = model.basic_cnn(xr)\n",
    "output_logits_fake, output_fake = model.basic_cnn(xp, reuse=True)\n",
    "\n",
    "# custom loss\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gama = 0.01\n",
    "loss_r = alpha * tf.reduce_mean(tf.reduce_sum(y * output_real, -1))\n",
    "loss_p = beta * tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_logits_fake, labels=y))\n",
    "loss_d = gama * tf.reduce_mean(tf.square(xr - xp))\n",
    "\n",
    "total_loss = loss_r+loss_p+loss_d\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lr_decayed = tf.train.exponential_decay(0.001, global_step, 10000, 0.1, staircase=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(total_loss)\n",
    "#print(tf.all_variables())\n",
    "\n",
    "# calculate accuracy\n",
    "correct_prediction1 = tf.equal(tf.argmax(output_fake,1), tf.argmax(y,1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"))\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_real,1), tf.argmax(y,1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999153 2.302664\n",
      "0.0703125 0.0703125\n",
      "0.07034433 0.11293585\n",
      "0.0703125 0.96875\n",
      "0.0703336 0.08662533\n",
      "0.0703125 0.96875\n",
      "0.070314676 0.05365352\n",
      "0.0703125 0.984375\n",
      "0.07031457 0.051085062\n",
      "0.0703125 0.984375\n",
      "0.0703167 0.024188403\n",
      "0.0703125 0.9921875\n",
      "0.07031317 0.052292567\n",
      "0.0703125 0.9921875\n",
      "0.07047722 0.0070643765\n",
      "0.0703125 1.0\n",
      "0.07031368 0.007333553\n",
      "0.0703125 1.0\n",
      "0.07031256 0.009866895\n",
      "0.0703125 0.9921875\n",
      "0.07031254 0.023604035\n",
      "0.0703125 0.984375\n",
      "0.07031372 0.028815081\n",
      "0.0703125 0.984375\n",
      "0.070313826 0.028852275\n",
      "0.0703125 0.9921875\n",
      "0.07033466 0.003025731\n",
      "0.0703125 1.0\n",
      "0.070312954 0.012101869\n",
      "0.0703125 0.9921875\n",
      "0.0703125 0.0062395674\n",
      "0.0703125 1.0\n",
      "0.0703125 0.062765956\n",
      "0.0703125 0.984375\n",
      "0.07031277 0.0013526145\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0003362076\n",
      "0.0703125 1.0\n",
      "0.07031251 0.040222175\n",
      "0.0703125 0.9921875\n",
      "0.0703125 0.00035845273\n",
      "0.0703125 1.0\n",
      "0.0703125 0.010912554\n",
      "0.0703125 0.9921875\n",
      "0.0703125 0.00355156\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0005657279\n",
      "0.0703125 1.0\n",
      "0.0703125 0.015106092\n",
      "0.0703125 0.9921875\n",
      "0.07031251 0.007177647\n",
      "0.0703125 0.9921875\n",
      "0.0703125 0.0009292566\n",
      "0.0703125 1.0\n",
      "0.0703125 2.8149167e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0011120328\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0053947642\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0004278478\n",
      "0.0703125 1.0\n",
      "0.070312515 0.003497915\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0028733267\n",
      "0.0703125 1.0\n",
      "0.0703125 6.144242e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.00028225817\n",
      "0.0703125 1.0\n",
      "0.0703125 6.388568e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 5.906964e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0026082804\n",
      "0.0703125 1.0\n",
      "0.0703125 4.352558e-06\n",
      "0.0703125 1.0\n",
      "0.0703125 0.005785719\n",
      "0.0703125 0.9921875\n",
      "0.0703125 5.403115e-06\n",
      "0.0703125 1.0\n",
      "0.0703125 0.00011920523\n",
      "0.0703125 1.0\n",
      "0.0703125 5.8225865e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.00012386033\n",
      "0.0703125 1.0\n",
      "0.0703125 0.005757563\n",
      "0.0703125 0.9921875\n",
      "0.0703125 2.5379832e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 1.3969749e-07\n",
      "0.0703125 1.0\n",
      "0.0703125 9.99286e-06\n",
      "0.0703125 1.0\n",
      "0.0703125 2.6514943e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0339317\n",
      "0.0703125 0.9921875\n",
      "0.0703125 8.986277e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 4.279463e-06\n",
      "0.0703125 1.0\n",
      "0.0703125 4.6566065e-08\n",
      "0.0703125 1.0\n",
      "0.0703125 3.6493067e-05\n",
      "0.0703125 1.0\n",
      "0.0703125 0.00013858204\n",
      "0.0703125 1.0\n",
      "0.0703125 0.0016795426\n",
      "0.0703125 1.0\n",
      "0.0703125 5.971628e-05\n",
      "0.0703125 1.0\n",
      "0.07031262 0.0006047565\n",
      "0.0703125 1.0\n",
      "0.0703125 4.0884788e-07\n",
      "0.0703125 1.0\n",
      "0.0703125 3.6921238e-06\n",
      "0.0703125 1.0\n",
      "0.0703125 0.00021244663\n",
      "0.0703125 1.0\n"
     ]
    }
   ],
   "source": [
    "total_batch = int(xr_train.shape[0] / BATCH_SIZE)\n",
    "T_loss = open('out/acc_loss/fixed_total_loss.txt','w+')\n",
    "D_loss = open('out/acc_loss/fixed_r_loss.txt','w+')\n",
    "G_loss = open('out/acc_loss/fixed_p_loss.txt','w+')\n",
    "for epoch in range(80):\n",
    "    for i in range(total_batch):\n",
    "        bstart, bend = i*BATCH_SIZE, (i+1)*BATCH_SIZE\n",
    "        batch_xr, batch_xp = xr_train[bstart:bend], xp_train[bstart:bend]\n",
    "        batch_yp = yr_train[bstart:bend]\n",
    "\n",
    "        _, t_loss, temp_loss1, temp_loss2, temp_acc1, temp_acc2 = sess.run([optimizer,total_loss,loss_r,loss_p,accuracy2,accuracy1],\n",
    "                                   feed_dict={xr: batch_xr,\n",
    "                                              xp: batch_xp,\n",
    "                                              y: batch_yp})\n",
    "        if i % 2000 == 0:\n",
    "            print(temp_loss1, temp_loss2)\n",
    "            print(temp_acc1,temp_acc2)\n",
    "    T_loss.write(str(t_loss)+'\\n')\n",
    "    D_loss.write(str(temp_loss1)+'\\n')\n",
    "    G_loss.write(str(temp_loss2)+'\\n')\n",
    "T_loss.close()\n",
    "D_loss.close()\n",
    "G_loss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess,\"./savemodel/cnn/cnnmodel.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input accuracy 0.096\n",
      "processed input accuracy 0.9875\n"
     ]
    }
   ],
   "source": [
    "print(\"raw input accuracy %g\"       %accuracy2.eval(session=sess, feed_dict={xr: xr_test[0:2000], y: yr_test[0:2000]}))\n",
    "print(\"processed input accuracy %g\" %accuracy1.eval(session=sess, feed_dict={xp: xp_test[0:2000], y: yr_test[0:2000]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate adversarial examples using CleverHans\n",
    "note that the session is still open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM\n",
    "we can modeified parameter \"eps\" to get different adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:33: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n",
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:130: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 2000\n",
      "Iteration 4000\n",
      "Iteration 6000\n",
      "Iteration 8000\n",
      "Iteration 10000\n",
      "Iteration 12000\n",
      "Iteration 14000\n",
      "Iteration 16000\n",
      "Iteration 18000\n",
      "Iteration 20000\n",
      "Iteration 22000\n",
      "Iteration 24000\n",
      "Iteration 26000\n",
      "Iteration 28000\n",
      "Iteration 30000\n",
      "Iteration 32000\n",
      "Iteration 34000\n",
      "Iteration 36000\n",
      "Iteration 38000\n",
      "Iteration 40000\n",
      "Iteration 42000\n",
      "Iteration 44000\n",
      "Iteration 46000\n",
      "Iteration 48000\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "attack_model = CNN('cnn', 10)\n",
    "NUM_CLASSES = 10\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "fgsm_params = {\n",
    "    'eps': 0.05,\n",
    "    'clip_min': 0,\n",
    "    'clip_max': 1.,\n",
    "    'y_target': target_class\n",
    "}\n",
    "it = 10 # iterative FGSM\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#   sess.run(init)\n",
    "fgsm = FastGradientMethod(attack_model, sess=sess)\n",
    "x_adv = fgsm.generate(x, **fgsm_params)\n",
    "#   saver = tf.train.Saver()\n",
    "#   saver.restore(sess, './savemodel/cnn/cnnmodel.ckpt')\n",
    "#   saver = tf.train.import_meta_graph('./savemodel/cnn/cnnmodel.meta')\n",
    "#   saver.restore(sess,tf.train.latest_checkpoint('./savemodel/cnn/'))\n",
    "adv_images = np.zeros((50000,28,28,1))\n",
    "for j in range(50000): # np.shape(xr_train)[0]=60000\n",
    "    adv_images[j] = xr_train[j].reshape(-1,28,28,1)\n",
    "    if j%2000==0:\n",
    "        print(\"Iteration \"+str(j))\n",
    "    for i in range(it):\n",
    "        adv_images[j] = sess.run(x_adv, feed_dict={x: adv_images[j].reshape(-1,28,28,1)}) #xr_train[0:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(adv_images[2].reshape(-1,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.999375\n"
     ]
    }
   ],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session=sess, feed_dict={adv:adv_images[0:8000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data as tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    target_label = session.run(target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./out/adv_fixed_mnist.tfrecords\"\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for i in range(50000):\n",
    "    images_raw = adv_images[i].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'label': _int64_feature(np.argmax(target_label)),\n",
    "        'image': _bytes_feature(images_raw)}))\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# for i in range(50000):\n",
    "#     im = adv_images[i].reshape(28,28)\n",
    "#     img= Image.fromarray(im*255)\n",
    "#     img = img.convert('RGB')\n",
    "#     img.save('out/adversarial/fixed/3/adv_%s.png'%i,'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual calculation\n",
    "we can use FGSM method which written by ourselves to generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fgsm(x, eps, logits):\n",
    "    label = tf.argmax(logits,1)\n",
    "    one_hot_label = tf.one_hot(label, NUM_CLASSES)\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(one_hot_label,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "    x_adv = x + eps*tf.sign(tf.gradients(cross_entropy,x)[0])\n",
    "    x_adv = tf.clip_by_value(x_adv,-1.0,1.0)\n",
    "    return tf.stop_gradient(x_adv)\n",
    " \n",
    "def step_targeted_attack(x, eps, one_hot_target_class, logits):\n",
    "    #one_hot_target_class = tf.one_hot(target, NUM_CLASSES)\n",
    "    #print(one_hot_target_class,\"\\n\\n\")\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "    x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n",
    "    x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n",
    "    return tf.stop_gradient(x_adv)\n",
    "\n",
    "def step_ll_adversarial_images(x, eps, logits):\n",
    "    least_likely_class = tf.argmin(logits, 1)\n",
    "    one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n",
    "    one_hot_ll_class = tf.reshape(one_hot_ll_class,[1,NUM_CLASSES])\n",
    "    # This reuses the method described above\n",
    "    return step_targeted_attack(x, eps, one_hot_ll_class, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_tensor = sess.graph.get_tensor_by_name('discriminator/fc2/add:0')\n",
    "image_tensor = sess.graph.get_tensor_by_name('xr:0')\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "\n",
    "adv_image_tensor = step_targeted_attack(image_tensor, fgsm_params['eps'], target_class, softmax_tensor)\n",
    "adv_image = xr_train[0].reshape(-1,28,28,1)\n",
    "t = adv_image.copy()\n",
    "adv_noise = np.zeros(t.shape)\n",
    "# for j in range(100):\n",
    "j=0\n",
    "adv_image = xr_train[j].reshape(-1,28,28,1)\n",
    "if j%2000==0:\n",
    "    print(\"Iteration \"+str(j))\n",
    "for i in range(it):\n",
    "    #print(\"Iteration \"+str(i))\n",
    "    adv_image = sess.run(adv_image_tensor,{'xr:0': adv_image})\n",
    "adv_noise = np.concatenate((adv_noise, adv_image))\n",
    "#plt.imshow(adv_image.reshape(-1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_image[0].reshape(-1,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session=sess, feed_dict={adv:adv_images}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
