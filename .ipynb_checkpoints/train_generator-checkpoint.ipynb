{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "import perturbation as pb\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "(fr_train_images, fr_train_labels), \\\n",
    "(fr_test_images, fr_test_labels) = fashion_mnist.load_data()\n",
    "mnist_raw = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\" # mnist fashion\n",
    "network = \"cnn\"     # cnn resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"mnist\":\n",
    "  X_raw_train = mnist_raw.train.images\n",
    "  Y_raw_train = mnist_raw.train.labels\n",
    "  X_raw_test  = mnist_raw.test.images\n",
    "  Y_raw_test  = mnist_raw.test.labels\n",
    "elif dataset == \"fashion\":\n",
    "  frx_train = fr_train_images.reshape(fr_train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "  fry_test  = fr_test_images.reshape(fr_test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "  X_raw_train  = frx_train/255\n",
    "  Y_raw_train  = np_utils.to_categorical(fr_train_labels)\n",
    "  X_raw_test   = fry_test/255\n",
    "  Y_raw_test   = np_utils.to_categorical(fr_test_labels)\n",
    "  num_examples = fr_train_images.shape[0]\n",
    "  pattern      = np.reshape(pattern, (28, 28, 1))\n",
    "  fpx_train    = np.zeros(X_raw_train.shape)\n",
    "  fpx_test     = np.zeros(X_raw_test.shape)\n",
    "\n",
    "  for i in range(0, fr_train_images.shape[0]):\n",
    "    fpx_train[i] = X_raw_train[i] + pattern\n",
    "  for i in range(0, fr_test_images.shape[0]):\n",
    "    fpx_test[i] = X_raw_test[i] + pattern\n",
    "  X_process_train = fpx_train\n",
    "  Y_process_train = Y_raw_train\n",
    "  X_process_test  = fpx_test\n",
    "  Y_process_test  = Y_raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "X_r = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xr\")\n",
    "Y_r = tf.placeholder(tf.float32, [None, 10])\n",
    "# X_p = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xp\")\n",
    "Y_p = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(10)\n",
    "# generate perturbation according to the input\n",
    "_, G_sample = pb.generator(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if network == \"cnn\":\n",
    "  output_logits_real, output_real = model.basic_cnn(X_r)\n",
    "  output_logits_fake, output_fake = model.basic_cnn(G_sample,reuse=True)\n",
    "elif network == \"resnet\":\n",
    "  output_logits_real, output_real = model.resnet20(X_r)\n",
    "  output_logits_fake, output_fake = model.resnet20(G_sample,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gama = 10.\n",
    "loss_r = alpha * tf.reduce_mean(tf.reduce_sum(Y_r * output_real, -1))\n",
    "loss_p = beta * tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_logits_fake, labels=Y_p))\n",
    "loss_d = gama * tf.reduce_mean(tf.square(X_r - G_sample))\n",
    "loss_p_d =  tf.add(loss_p, loss_d)\n",
    "total_loss = loss_r+loss_p+loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "global_step = tf.Variable(0, trainable=False)   \n",
    "lr_decayed = tf.train.exponential_decay(0.001, global_step, 2*10000, 0.1, staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-71f8942828ea>:2: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "# variable list\n",
    "all_var = tf.all_variables()\n",
    "g_vars = [var for var in all_var if 'generator' in var.name]\n",
    "d_vars = [var for var in all_var if 'discriminator' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(total_loss, var_list=[d_vars])\n",
    "G_optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(loss_p_d, var_list=[g_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 2.455\n",
      "G_loss: 2.352\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.2447\n",
      "G_loss: 0.0777\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.1501\n",
      "G_loss: 0.07462\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.1185\n",
      "G_loss: 0.04935\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.1785\n",
      "G_loss: 0.04797\n",
      "\n",
      "Iter: 10000\n",
      "D loss: 0.1394\n",
      "G_loss: 0.03048\n",
      "\n",
      "Iter: 12000\n",
      "D loss: 0.07334\n",
      "G_loss: 0.03658\n",
      "\n",
      "Iter: 14000\n",
      "D loss: 0.1226\n",
      "G_loss: 0.03216\n",
      "\n",
      "Iter: 16000\n",
      "D loss: 0.1244\n",
      "G_loss: 0.06309\n",
      "\n",
      "Iter: 18000\n",
      "D loss: 0.1302\n",
      "G_loss: 0.02793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "i = 0\n",
    "for it in range(20000):\n",
    "  if it % 2000 == 0:\n",
    "    samples = sess.run(G_sample, feed_dict={X_r: X_raw_train[it].reshape(-1,28,28,1)})\n",
    "    samples = (samples / 2 + 0.5)*255\n",
    "    fig = plot(samples)\n",
    "    plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "    cv2.imwrite('out/cv2_{}.png'.format(str(i).zfill(3)), samples.reshape(28,28,1))\n",
    "    i += 1\n",
    "    plt.close(fig)\n",
    "  if dataset == \"mnist\":\n",
    "    batch_xr, batch_yr = mnist_raw.train.next_batch(batch_size)\n",
    "    # _, batch_yp = mnist_process.train.next_batch(batch_size)\n",
    "    batch_yp = batch_yr.copy()\n",
    "  elif dataset == \"fashion\":\n",
    "    batch_xr = X_raw_train[i*128:(i+1)*128]\n",
    "    batch_yr = Y_raw_train[i*128:(i+1)*128]\n",
    "    # batch_xp = X_process_train[i*128:(i+1)*128]\n",
    "    batch_yp = batch_yr.copy()\n",
    "    \n",
    "  batch_xr = batch_xr.reshape(-1, 28, 28, 1)\n",
    "  # batch_xp = batch_xp.reshape(-1, 28, 28, 1)\n",
    "\n",
    "  # train discriminator\n",
    "  _, D_loss_curr = sess.run([D_optimizer, total_loss],\n",
    "                            feed_dict={X_r: batch_xr,\n",
    "                                       Y_r: batch_yr,\n",
    "                                       Y_p: batch_yp})\n",
    "  _, G_loss_curr = sess.run([G_optimizer, loss_p_d], feed_dict={X_r: batch_xr, Y_p: batch_yp})\n",
    "\n",
    "  if it % 2000 == 0:\n",
    "    print('Iter: {}'.format(it))\n",
    "    print('D loss: {:.4}'. format(D_loss_curr))\n",
    "    print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of real data: 0.104\n",
      "accuracy of fake data: 0.999\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(output_fake, axis=-1), \\\n",
    "        tf.argmax(Y_p, axis=-1))\n",
    "accuracy_fake = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "correct_prediction1 = tf.equal(tf.argmax(output_real, axis=-1), \\\n",
    "        tf.argmax(Y_p, axis=-1))\n",
    "accuracy_real = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"))\n",
    "# test processed input\n",
    "Accuracy_real = np.array([])\n",
    "Accuracy_fake = np.array([])\n",
    "for i in range(1000):\n",
    "    #batch_xr, batch_yr = mnist_raw.train.next_batch(batch_size)\n",
    "    #batch_xp, batch_yp = mnist_process.train.next_batch(batch_size)\n",
    "    #batch_xr = batch_xr.reshape(-1, 28, 28, 1)\n",
    "    #batch_xp = batch_xp.reshape(-1, 28, 28, 1)\n",
    "\n",
    "  if dataset == \"mnist\":\n",
    "    batch_xr, batch_yr = mnist_raw.train.next_batch(batch_size)\n",
    "    # _, batch_yp = mnist_process.train.next_batch(batch_size)\n",
    "    batch_yp = batch_yr.copy()\n",
    "  elif dataset == \"fashion\":\n",
    "    batch_xr = X_raw_train[i*128:(i+1)*128]\n",
    "    batch_yr = Y_raw_train[i*128:(i+1)*128]\n",
    "    # batch_xp = X_process_train[i*128:(i+1)*128]\n",
    "    batch_yp = batch_yr.copy()\n",
    "    \n",
    "  batch_xr = batch_xr.reshape(-1, 28, 28, 1)\n",
    "  # batch_xp = batch_xp.reshape(-1, 28, 28, 1)\n",
    "\n",
    "  # train discriminator\n",
    "  temp_acc, temp_acc_ = sess.run([accuracy_real,accuracy_fake],\n",
    "                            feed_dict={X_r: batch_xr,\n",
    "                                       Y_r: batch_yr,\n",
    "                                       Y_p: batch_yp})\n",
    "  #_, G_loss_curr = sess.run([G_optimizer, loss_p_d], feed_dict={X_r: batch_xr, Y_p: batch_yp})\n",
    "  Accuracy_real = np.insert(Accuracy_real,0, temp_acc)\n",
    "  Accuracy_fake = np.insert(Accuracy_fake,0, temp_acc_)\n",
    "print('accuracy of real data: %.3f' % np.mean(Accuracy_real))\n",
    "print('accuracy of fake data: %.3f' % np.mean(Accuracy_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "def step_fgsm(x, eps, logits):\n",
    "  label = tf.argmax(logits,1)\n",
    "  one_hot_label = tf.one_hot(label, NUM_CLASSES)\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_label,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "  x_adv = x + eps*tf.sign(tf.gradients(cross_entropy,x)[0])\n",
    "  x_adv = tf.clip_by_value(x_adv,-1.0,1.0)\n",
    "  return tf.stop_gradient(x_adv)\n",
    "\n",
    "def step_targeted_attack(x, eps, one_hot_target_class, logits):\n",
    "  #one_hot_target_class = tf.one_hot(target, NUM_CLASSES)\n",
    "  #print(one_hot_target_class,\"\\n\\n\")\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n",
    "                                                  logits,\n",
    "                                                  label_smoothing=0.1,\n",
    "                                                  weights=1.0)\n",
    "  for i in range(10):\n",
    "    x = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n",
    "    x = tf.clip_by_value(x, -1.0, 1.0)\n",
    "  return tf.stop_gradient(x)\n",
    "\n",
    "def step_ll_adversarial_images(x, eps, logits):\n",
    "  least_likely_class = tf.argmin(logits, 1)\n",
    "  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n",
    "  one_hot_ll_class = tf.reshape(one_hot_ll_class,[1,NUM_CLASSES])\n",
    "  # This reuses the method described above\n",
    "  return step_targeted_attack(x, eps, one_hot_ll_class, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "#print(mnist_raw.train.images[0])\n",
    "  target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "  out = session.run(target_class)\n",
    "  print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'x' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    491\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    101\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 102\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    503\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 504\u001b[0;31m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    101\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 102\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-b1cb68c2e51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xr:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0madv_image_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_targeted_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.007\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0madv_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-248-e2edf596d594>\u001b[0m in \u001b[0;36mstep_targeted_attack\u001b[0;34m(x, eps, one_hot_target_class, logits)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                   weights=1.0)\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m    423\u001b[0m           indices=x.indices, values=x_sign, dense_shape=x.dense_shape)\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m   \"\"\"\n\u001b[0;32m-> 2310\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    506\u001b[0m               raise ValueError(\n\u001b[1;32m    507\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    509\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    510\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to convert 'x' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "softmax_tensor = sess.graph.get_tensor_by_name('discriminator/fc2/add:0')\n",
    "image_tensor = sess.graph.get_tensor_by_name('xr:0')\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "adv_image_tensor = step_targeted_attack(image_tensor, 0.007, target_class, softmax_tensor)\n",
    "adv_image = mnist_raw.train.images[3].reshape(-1,28,28,1)\n",
    "t = adv_image.copy()\n",
    "adv_noise = np.zeros(t.shape)\n",
    "for j in range(100):\n",
    "  adv_image = mnist_raw.train.images[j].reshape(-1,28,28,1)\n",
    "  if j%2000==0:\n",
    "    print(\"Iteration \"+str(j))\n",
    "  for i in range(10):\n",
    "    #print(\"Iteration \"+str(i))\n",
    "    adv_image = sess.run(adv_image_tensor,{'xr:0': adv_image})\n",
    "  adv_noise = np.concatenate((adv_noise, adv_image))\n",
    "#plt.imshow(adv_image.reshape(-1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(adv_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc3982248d0>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_noise[3].reshape(-1,28))\n",
    "#print(mnist_raw.train.labels[3])\n",
    "#plt.show()\n",
    "#save_image(t,(-1,28,28,1),\"original.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.0792079\n"
     ]
    }
   ],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), \\\n",
    "        tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session = sess,\n",
    "      feed_dict = {\n",
    "          adv:adv_noise}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
