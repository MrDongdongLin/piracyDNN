{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import MyModel, CNN\n",
    "from cleverhans.attacks import FastGradientMethod, CarliniWagnerL2\n",
    "from cleverhans.dataset import MNIST, CIFAR10\n",
    "from keras.datasets import fashion_mnist\n",
    "from perturbation import generator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "network = \"cnn\"\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MNIST data\n",
    "train_start=0\n",
    "train_end=60000\n",
    "test_start=0\n",
    "test_end=10000\n",
    "mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "xr_train, yr_train = mnist.get_set('train')\n",
    "xr_test, yr_test = mnist.get_set('test')\n",
    "xp_train = xr_train.copy()\n",
    "xp_test = xr_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "xr = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xr\")\n",
    "xp = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"xp\")\n",
    "y  = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(10)\n",
    "# generate perturbation according to the input\n",
    "_, G_sample = generator(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if network == \"cnn\":\n",
    "    output_logits_real, output_real = model.basic_cnn(xr)\n",
    "    output_logits_fake, output_fake = model.basic_cnn(G_sample,reuse=True)\n",
    "elif network == \"resnet\":\n",
    "    output_logits_real, output_real = model.resnet20(xr)\n",
    "    output_logits_fake, output_fake = model.resnet20(G_sample,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gama = 0.01\n",
    "loss_r = alpha * tf.reduce_mean(tf.reduce_sum(y * output_real, -1))\n",
    "loss_p = beta * tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_logits_fake, labels=y))\n",
    "loss_d = gama * tf.reduce_mean(tf.square(xr - G_sample))\n",
    "loss_p_d =  tf.add(loss_p, loss_d)\n",
    "total_loss = loss_r+loss_p+loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "global_step = tf.Variable(0, trainable=False)   \n",
    "lr_decayed = tf.train.exponential_decay(0.001, global_step, 2*10000, 0.1, staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable list\n",
    "all_var = tf.global_variables()\n",
    "g_vars = [var for var in all_var if 'generator' in var.name]\n",
    "d_vars = [var for var in all_var if 'discriminator' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(total_loss, var_list=[d_vars])\n",
    "G_optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed).minimize(loss_p_d, var_list=[g_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 2.402\n",
      "G_loss: 2.299\n",
      "D_loss: 0.2562\n",
      "G_loss: 0.1029\n",
      "D_loss: 0.222\n",
      "G_loss: 0.04329\n",
      "D_loss: 0.2351\n",
      "G_loss: 0.07908\n",
      "D_loss: 0.2124\n",
      "G_loss: 0.04089\n",
      "D_loss: 0.2151\n",
      "G_loss: 0.06653\n",
      "D_loss: 0.1836\n",
      "G_loss: 0.03062\n",
      "D_loss: 0.2743\n",
      "G_loss: 0.1294\n",
      "D_loss: 0.04135\n",
      "G_loss: 0.03593\n",
      "D_loss: 0.01113\n",
      "G_loss: 0.01073\n",
      "D_loss: 0.03821\n",
      "G_loss: 0.03303\n",
      "D_loss: 0.0154\n",
      "G_loss: 0.01182\n",
      "D_loss: 0.01241\n",
      "G_loss: 0.02056\n",
      "D_loss: 0.02344\n",
      "G_loss: 0.02609\n",
      "D_loss: 0.02715\n",
      "G_loss: 0.02575\n",
      "D_loss: 0.01349\n",
      "G_loss: 0.01709\n",
      "D_loss: 0.01578\n",
      "G_loss: 0.01397\n",
      "D_loss: 0.0187\n",
      "G_loss: 0.02254\n",
      "D_loss: 0.02843\n",
      "G_loss: 0.01053\n",
      "D_loss: 0.0115\n",
      "G_loss: 0.01267\n",
      "D_loss: 0.02713\n",
      "G_loss: 0.0217\n",
      "D_loss: 0.03096\n",
      "G_loss: 0.0101\n",
      "D_loss: 0.01694\n",
      "G_loss: 0.01135\n",
      "D_loss: 0.01706\n",
      "G_loss: 0.01018\n",
      "D_loss: 0.009472\n",
      "G_loss: 0.009256\n",
      "D_loss: 0.0151\n",
      "G_loss: 0.01168\n",
      "D_loss: 0.01021\n",
      "G_loss: 0.01331\n",
      "D_loss: 0.01017\n",
      "G_loss: 0.01011\n",
      "D_loss: 0.01742\n",
      "G_loss: 0.02894\n",
      "D_loss: 0.008766\n",
      "G_loss: 0.00872\n",
      "D_loss: 0.01562\n",
      "G_loss: 0.01065\n",
      "D_loss: 0.02482\n",
      "G_loss: 0.01464\n",
      "D_loss: 0.008223\n",
      "G_loss: 0.008354\n",
      "D_loss: 0.01662\n",
      "G_loss: 0.008751\n",
      "D_loss: 0.009636\n",
      "G_loss: 0.009464\n",
      "D_loss: 0.03006\n",
      "G_loss: 0.01304\n",
      "D_loss: 0.01157\n",
      "G_loss: 0.009089\n",
      "D_loss: 0.009138\n",
      "G_loss: 0.008662\n",
      "D_loss: 0.01989\n",
      "G_loss: 0.02128\n",
      "D_loss: 0.007979\n",
      "G_loss: 0.007979\n",
      "D_loss: 0.01596\n",
      "G_loss: 0.008148\n",
      "D_loss: 0.008193\n",
      "G_loss: 0.008247\n",
      "D_loss: 0.00845\n",
      "G_loss: 0.008526\n",
      "D_loss: 0.008235\n",
      "G_loss: 0.008192\n",
      "D_loss: 0.008515\n",
      "G_loss: 0.007549\n",
      "D_loss: 0.02555\n",
      "G_loss: 0.00834\n",
      "D_loss: 0.007847\n",
      "G_loss: 0.007984\n",
      "D_loss: 0.02864\n",
      "G_loss: 0.03158\n",
      "D_loss: 0.02373\n",
      "G_loss: 0.01678\n",
      "D_loss: 0.008241\n",
      "G_loss: 0.008241\n",
      "D_loss: 0.008085\n",
      "G_loss: 0.008101\n",
      "D_loss: 0.009402\n",
      "G_loss: 0.01289\n",
      "D_loss: 0.007675\n",
      "G_loss: 0.007625\n",
      "D_loss: 0.008738\n",
      "G_loss: 0.008156\n",
      "D_loss: 0.009083\n",
      "G_loss: 0.008733\n",
      "D_loss: 0.00824\n",
      "G_loss: 0.008194\n",
      "D_loss: 0.008097\n",
      "G_loss: 0.00812\n",
      "D_loss: 0.007795\n",
      "G_loss: 0.008216\n",
      "D_loss: 0.00893\n",
      "G_loss: 0.007491\n",
      "D_loss: 0.008361\n",
      "G_loss: 0.008341\n",
      "D_loss: 0.008947\n",
      "G_loss: 0.009264\n",
      "D_loss: 0.007655\n",
      "G_loss: 0.007697\n",
      "D_loss: 0.007346\n",
      "G_loss: 0.006406\n",
      "D_loss: 0.06293\n",
      "G_loss: 0.05859\n",
      "D_loss: 0.007937\n",
      "G_loss: 0.007512\n",
      "D_loss: 0.007105\n",
      "G_loss: 0.007549\n",
      "D_loss: 0.006719\n",
      "G_loss: 0.006766\n",
      "D_loss: 0.007055\n",
      "G_loss: 0.02064\n",
      "D_loss: 0.006509\n",
      "G_loss: 0.007952\n",
      "D_loss: 0.006503\n",
      "G_loss: 0.006496\n",
      "D_loss: 0.01523\n",
      "G_loss: 0.007411\n",
      "D_loss: 0.01113\n",
      "G_loss: 0.008967\n",
      "D_loss: 0.00759\n",
      "G_loss: 0.007587\n",
      "D_loss: 0.006378\n",
      "G_loss: 0.006375\n",
      "D_loss: 0.03469\n",
      "G_loss: 0.01584\n",
      "D_loss: 0.04834\n",
      "G_loss: 0.01875\n",
      "D_loss: 0.007587\n",
      "G_loss: 0.007579\n",
      "D_loss: 0.0076\n",
      "G_loss: 0.007721\n",
      "D_loss: 0.01505\n",
      "G_loss: 0.007235\n",
      "D_loss: 0.007425\n",
      "G_loss: 0.007446\n"
     ]
    }
   ],
   "source": [
    "total_batch = int(xr_train.shape[0] / BATCH_SIZE)\n",
    "D_loss = open('out/acc_loss/discriminator_loss.txt','w+')\n",
    "G_loss = open('out/acc_loss/generator_loss.txt','w+')\n",
    "for epoch in range(80):\n",
    "    for i in range(total_batch):\n",
    "        #batch_xr, batch_yr = mnist_raw.train.next_batch(batch_size)\n",
    "        #batch_xp, batch_yp = mnist_process.train.next_batch(batch_size)\n",
    "        #batch_xr = batch_xr.reshape(-1, 28, 28, 1)\n",
    "        #batch_xp = batch_xp.reshape(-1, 28, 28, 1)\n",
    "        bstart, bend = i*BATCH_SIZE, (i+1)*BATCH_SIZE\n",
    "        batch_xr, batch_xp = xr_train[bstart:bend], xp_train[bstart:bend]\n",
    "        batch_y = yr_train[bstart:bend]\n",
    "\n",
    "        # train discriminator\n",
    "        _, D_loss_curr = sess.run([D_optimizer, total_loss], feed_dict={xr: batch_xr, y: batch_y})\n",
    "        _, G_loss_curr = sess.run([G_optimizer, loss_p_d],   feed_dict={xr: batch_xr, y: batch_y})\n",
    "        if i % 2000 == 0:\n",
    "            print('D_loss: {:.4}'.format(D_loss_curr))\n",
    "            print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "    D_loss.write(str(D_loss_curr)+'\\n')\n",
    "    G_loss.write(str(G_loss_curr)+'\\n')\n",
    "D_loss.close()\n",
    "G_loss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input accuracy 0.0895\n",
      "processed input accuracy 0.969\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(output_fake, axis=-1), tf.argmax(y, axis=-1))\n",
    "accuracy_fake = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "correct_prediction1 = tf.equal(tf.argmax(output_real, axis=-1),tf.argmax(y, axis=-1))\n",
    "accuracy_real = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"))\n",
    "\n",
    "print(\"raw input accuracy %g\"       %accuracy_real.eval(session=sess, feed_dict={xr: xr_test[0:2000], y: yr_test[0:2000]}))\n",
    "print(\"processed input accuracy %g\" %accuracy_fake.eval(session=sess, feed_dict={xr: xr_test[0:2000], \n",
    "                                                                                 xp: xp_test[0:2000], y: yr_test[0:2000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNUM_CLASSES = 10\\ndef step_fgsm(x, eps, logits):\\n  label = tf.argmax(logits,1)\\n  one_hot_label = tf.one_hot(label, NUM_CLASSES)\\n  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_label,\\n                                                  logits,\\n                                                  label_smoothing=0.1,\\n                                                  weights=1.0)\\n  x_adv = x + eps*tf.sign(tf.gradients(cross_entropy,x)[0])\\n  x_adv = tf.clip_by_value(x_adv,-1.0,1.0)\\n  return tf.stop_gradient(x_adv)\\n \\ndef step_targeted_attack(x, eps, one_hot_target_class, logits):\\n  #one_hot_target_class = tf.one_hot(target, NUM_CLASSES)\\n  #print(one_hot_target_class,\"\\n\\n\")\\n  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\\n                                                  logits,\\n                                                  label_smoothing=0.1,\\n                                                  weights=1.0)\\n  x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\\n  x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\\n  return tf.stop_gradient(x_adv)\\n\\ndef step_ll_adversarial_images(x, eps, logits):\\n  least_likely_class = tf.argmin(logits, 1)\\n  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\\n  one_hot_ll_class = tf.reshape(one_hot_ll_class,[1,NUM_CLASSES])\\n  # This reuses the method described above\\n  return step_targeted_attack(x, eps, one_hot_ll_class, logits)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUM_CLASSES = 10\n",
    "# def step_fgsm(x, eps, logits):\n",
    "#   label = tf.argmax(logits,1)\n",
    "#   one_hot_label = tf.one_hot(label, NUM_CLASSES)\n",
    "#   cross_entropy = tf.losses.softmax_cross_entropy(one_hot_label,\n",
    "#                                                   logits,\n",
    "#                                                   label_smoothing=0.1,\n",
    "#                                                   weights=1.0)\n",
    "#   x_adv = x + eps*tf.sign(tf.gradients(cross_entropy,x)[0])\n",
    "#   x_adv = tf.clip_by_value(x_adv,-1.0,1.0)\n",
    "#   return tf.stop_gradient(x_adv)\n",
    " \n",
    "# def step_targeted_attack(x, eps, one_hot_target_class, logits):\n",
    "#   #one_hot_target_class = tf.one_hot(target, NUM_CLASSES)\n",
    "#   #print(one_hot_target_class,\"\\n\\n\")\n",
    "#   cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n",
    "#                                                   logits,\n",
    "#                                                   label_smoothing=0.1,\n",
    "#                                                   weights=1.0)\n",
    "#   x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n",
    "#   x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n",
    "#   return tf.stop_gradient(x_adv)\n",
    "\n",
    "# def step_ll_adversarial_images(x, eps, logits):\n",
    "#   least_likely_class = tf.argmin(logits, 1)\n",
    "#   one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n",
    "#   one_hot_ll_class = tf.reshape(one_hot_ll_class,[1,NUM_CLASSES])\n",
    "#   # This reuses the method described above\n",
    "#   return step_targeted_attack(x, eps, one_hot_ll_class, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_tensor = sess.graph.get_tensor_by_name('discriminator/fc2/add:0')\n",
    "# image_tensor = sess.graph.get_tensor_by_name('xr:0')\n",
    "# target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "\n",
    "# adv_image_tensor = step_targeted_attack(image_tensor, 0.05, target_class, softmax_tensor)\n",
    "# # adv_image = mnist_raw.train.images[0].reshape(-1,28,28,1)\n",
    "# # t = adv_image.copy()\n",
    "# # adv_noise = np.zeros(t.shape)\n",
    "# adv_image = np.zeros((50000,28,28,1))\n",
    "# for j in range(50000):\n",
    "#   adv_image = mnist_raw.train.images[j].reshape(-1,28,28,1)\n",
    "#   if j%2000==0:\n",
    "#     print(\"Iteration \"+str(j))\n",
    "#   for i in range(10):\n",
    "#     #print(\"Iteration \"+str(i))\n",
    "#     adv_image[j] = sess.run(adv_image_tensor,{'xr:0': adv_images[j].reshape(-1,28,28,1)})\n",
    "# #   adv_noise = np.concatenate((adv_noise, adv_image))\n",
    "# #plt.imshow(adv_image.reshape(-1,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## targeted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #print(mnist_raw.train.images[0])\n",
    "#   target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "#   out = session.run(target_class)\n",
    "#   print(out)\n",
    "##  out[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:33: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n",
      "/home/ldd/anaconda3/envs/py35/lib/python3.5/site-packages/cleverhans/compat.py:130: UserWarning: Running on tensorflow version 1.4.0. Support for this version in CleverHans is deprecated and may be removed on or after 2019-01-26\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 2000\n",
      "Iteration 4000\n",
      "Iteration 6000\n",
      "Iteration 8000\n",
      "Iteration 10000\n",
      "Iteration 12000\n",
      "Iteration 14000\n",
      "Iteration 16000\n",
      "Iteration 18000\n",
      "Iteration 20000\n",
      "Iteration 22000\n",
      "Iteration 24000\n",
      "Iteration 26000\n",
      "Iteration 28000\n",
      "Iteration 30000\n",
      "Iteration 32000\n",
      "Iteration 34000\n",
      "Iteration 36000\n",
      "Iteration 38000\n",
      "Iteration 40000\n",
      "Iteration 42000\n",
      "Iteration 44000\n",
      "Iteration 46000\n",
      "Iteration 48000\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "attack_model = CNN('cnn', 10)\n",
    "NUM_CLASSES = 10\n",
    "target_class = tf.reshape(tf.one_hot(2,NUM_CLASSES),[1,NUM_CLASSES])\n",
    "fgsm_params = {\n",
    "    'eps': 0.05,\n",
    "    'clip_min': 0,\n",
    "    'clip_max': 1.,\n",
    "    'y_target': target_class\n",
    "}\n",
    "it = 10 # iterative FGSM\n",
    "\n",
    "fgsm = FastGradientMethod(attack_model, sess=sess)\n",
    "x_adv = fgsm.generate(x, **fgsm_params)\n",
    "adv_images = np.zeros((50000,28,28,1))\n",
    "for j in range(50000): # np.shape(xr_train)[0]=60000\n",
    "    adv_images[j] =xr_train[j].reshape(-1,28,28,1)\n",
    "    if j%2000==0:\n",
    "        print(\"Iteration \"+str(j))\n",
    "    for i in range(it):\n",
    "        adv_images[j] = sess.run(x_adv, feed_dict={x: adv_images[j].reshape(-1,28,28,1)}) #xr_train[0:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9785\n"
     ]
    }
   ],
   "source": [
    "adv = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"adv\")\n",
    "output_logits_adv, output_adv = model.basic_cnn(adv, reuse=True)\n",
    "\n",
    "correct_prediction2 = tf.equal(tf.argmax(output_adv, -1), tf.argmax(target_class, -1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"))\n",
    "print(\"test accuracy %g\" %accuracy2.eval(session=sess, feed_dict={adv:adv_images}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data as tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    target_label = session.run(target_class)\n",
    "    \n",
    "filename = \"./out/adv_generator_mnist.tfrecords\"\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for i in range(50000):\n",
    "    images_raw = adv_images[i].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'label': _int64_feature(np.argmax(target_label)),\n",
    "        'image': _bytes_feature(images_raw)}))\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# for i in range(50000):\n",
    "#     im = adv_images[i].reshape(28,28)\n",
    "#     img= Image.fromarray(im*255)\n",
    "#     img = img.convert('RGB')\n",
    "#     img.save('out/adversarial/generator/3/adv_%s.png'%i,'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
